
王伟超
  wangweichao@tedu.cn
1、网络爬虫
  1、定义 ：网络蜘蛛、网络机器人,抓取网络数据的程序
  2、总结 ：用Python程序去模仿人去访问网站,模仿的越逼真越好
  3、爬取数据的目的 ：通过有效的大量数据分析市场走势、公司决策
2、企业获取数据的方式
  1、公司自有数据
  2、第三方数据平台购买
    数据堂、贵阳大数据交易所
  3、爬虫爬取数据
    市场上没有或者价格太高,利用爬虫程序爬取
3、Python做爬虫优势
  请求模块、解析模块丰富成熟,强大的scrapy框架
  PHP ：对多线程、异步支持不太好
  JAVA ：代码笨重,代码量很大
  C/C++ ：虽然效率高,但是代码成型很慢
4、爬虫分类
  1、通用网络爬虫(搜索引擎引用,需要遵守robots协议)
    http://www.taobao.com/robots.txt
    1、搜索引擎如何获取一个新网站的URL
      1、网站主动向搜索引擎提供(百度站长平台)
      2、和DNS服务网(万网),快速收录新网站
  2、聚焦网络爬虫
    自己写的爬虫程序：面向主题的爬虫、面向需求的爬虫
5、爬取数据步骤
  1、确定需要爬取的URL地址
  2、通过HTTP/HTTPS协议来获取相应的HTML页面
  3、提取HTML页面有用的数据
    1、所需数据,保存
    2、页面中有其他的URL,继续 第2步
6、Anaconda和Spyder
  1、Anaconda ：开源的Python发行版本
  2、Spyder ：集成开发环境
    Spyder常用快捷键：
      1、注释/取消注释 ：ctrl + 1
      2、保存 ：ctrl + s
      3、运行程序 ：f5
      4、自动补全 ：Tab
7、Chrome浏览器插件
  1、安装步骤
    1、右上角 - 更多工具 - 扩展程序
    2、点开右上角 - 开发者模式
    3、把插件 拖拽到 浏览器页面,释放鼠标,点击 添加扩展...
  2、插件介绍
    1、Proxy SwitchOmega ：代理切换插件
    2、Xpath Helper ：网页数据解析插件
    3、JSON View    ：查看json格式的数据(好看)
8、Fiddler抓包工具
  1、抓包工具设置
    1、Tools->options->HTTPS->...from browers only
    2、connections ：设置端口号 8888
  2、设置浏览器代理
    Proxy SwitchOmega -> 选项 -> 新建情景模式 -> HTTP 127.0.0.1 8888 -> 应用选项

    浏览器右上角图标 -> AID1806 -> 访问百度
9、WEB
  1、HTTP和HTTS
    1、HTTP ：80
    2、HTTPS ：443,HTTP的升级版,加了一个安全套接层
  2、GET和POST
    1、GET ：查询参数都会在URL上显示出来
    2、POST：查询参数和需要提交数据是隐藏在Form表单里的,不会再URL地址上显示
  3、URL ：统一资源定位符
    https://  item.jd.com  :80   /26809408972.html #detail
     协议     域名/IP地址  端口  访问资源的路径    锚点
  4、User-Agent
    记录用户的浏览器、操作系统等,为了让用户获取更好的HTML页面效果
    User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/69.0.3497.100 Safari/537.36

    Mozilla Firefox ：(Gecko内核)
    IE ：Trident(自己的内核)
    Linux : KTHML(like Gecko)
    Apple ：Webkit(like KHTML)
    Google：Chrome(like Webkit)
    其他浏览器都是模仿IE/Chrome
10、爬虫请求模块
  1、版本
    1、python2 ：urllib2、urllib
    2、python3 ：把urllib和urllib2合并,urllib.request
  2、常用方法
    1、urllib.request.urlopen("网址")
      1、作用 ：向网站发起一个请求并获取响应
        字节流 = response.read()
        字符串 = response.read().decode("utf-8")

        encode() : 字符串 --> bytes
        decode() : bytes  --> 字符串
      2、重构User-Agent
        1、不支持重构User-Agent ：urlopen()
        2、支持重构User-Agent
          urllib.request.Request(添加User-Agent)
    2、urllib.request.Request("网址",headers="字典")
      User-Agent是爬虫和反爬虫斗争的第一步,发送请求必须带User-Agent
      1、使用流程(见 02_urllib.request.Request.py)
        1、利用Request()方法构建请求对象
      	2、利用urlopen()方法获取响应对象
      	3、利用响应对象的read().decode("utf-8")获取内容
      2、响应对象response的方法
        1、read() ：读取服务器响应的内容
      	2、getcode()
      	  1、作用
      	    返回HTTP的响应码
      	      print(respones.getcode())
      	      200 ：成功
      	      4XX ：服务器页面出错
      	      5XX ：服务器出错
        3、geturl()
          1、作用 ：返回实际数据的URL(防止重定向问题)
    3、urllib.parse模块
      1、urlencode(字典)  ## 注意：参数一定要为字典
         urlencode({"wd":"美女"})
         wd=%e8.......
         tedu = {"wd":"达内科技"}
         示例见 ：04_urllib.parse.urlencode示例.py
      	 请输入你要搜索的内容：美女
      	 保存到本地文件 ：美女.html
      2、quote(字符串) 见 05_urllib.parse.quote示例.py
        key = urllib.parse.quote("字符串")
        baseurl = "http://www.baidu.com/s?wd="
        key = input("请输入要搜索的内容:")
        #进行quote()编码
        key = urllib.parse.quote(key)
        url = baseurl + key
        print(url)
  4、练习
    百度贴吧数据抓取
    要求：
      1、输入要抓取的贴吧名称
      2、输入爬取的起始页和终止页
      3、把每一页的内容保存到本地
        第1页.html 第2页.html ... ... 
    步骤：
      1、找URL规律,拼接URL
        第1页：http://tieba.baidu.com/f?kw=??&pn=0
      	第2页：http://tieba.baidu.com/f?kw=??&pn=50
      	第3页：http://tieba.baidu.com/f?kw=??&pn=100
      	.....
      	第n页：pn=(n-1)*50
      2、获取网页内容(发请求获响应)
      3、保存(本地文件、数据库)
11、请求方式及实例
  1、GET
    1、特点 ：查询参数在URL地址中显示
    2、案例 ：抓取百度贴吧
  2、POST(在Request方法中添加data参数)
    1、urllib.request.Request(url,data=data,headers=headers)
      data ：表单数据以bytes类型提交,不能是str
    2、处理表单数据为bytes类型
      1、把Form表单数据定义为字典data
      2、urlencode(data)
      3、转为bytes数据类型 ：bytes()
    3、有道翻译案例
    4、有道翻译返回的是json格式的字符串,如何把json格式的字符串转换为Python中字典
      import json

      r_dict = json.loads(r_json)

*****************************
Day02笔记
1、解析
  1、数据的分类
    1、结构化数据
      特点 ：有固定的格式,如 ：HTML、XML、JSON
    2、非结构化数据
      示例 ：图片、音频、视频,这类数据一般都存储为二进制
  2、正则表达式re
    1、使用流程
      1、创建编译对象 ：p = re.compile("正则表达式")
      2、对字符串匹配 ：r = p.match("字符串")
      3、获取匹配结果 ：print(r.group())
    2、常用方法
      1、match(s) ：字符串开头的第1个,返回对象
      2、search(s)：从开始往后找,匹配第1个,返回对象
      3、group()  ：从match或search返回对象中取值
      4、findall()：所有全部匹配,返回一个列表
    3、表达式
      .  匹配任意字符(不能匹配\n)
      \d 数字
      \s 空白字符
      \S 非空白字符  
      [...] 包含[]内容 ：A[BCD]E  --> ABE  ACE  ADE 
      \w 字母、数字、_

      *  0次或多次
      ?  0次或1次
      +  1次或多次
      {m} m次
      {m,n} m-n次  AB{1,3}C --> ABC ABBC ABBBC

      贪婪匹配(.*) ：在整个表达式匹配成功的前提下,尽可能多的匹配*

      非贪婪匹配(.*?) ：在整个表达式匹配成功的前提下,尽可能少的匹配*
    4、示例(贪婪模式和非贪婪模式)
      见 ：见01_贪婪匹配和非贪婪匹配示例.py
    5、findall()的分组
      import re
        #解释 ：先按照整体匹配出来,然后再匹配()中的
        # 如果有2个或者多个(),则以元组的方式取显示

      s = "A B C D"
      p1 = re.compile('\w+\s+\w+')
      print(p1.findall(s))
      # ['A B','C D']

      p2 = re.compile('(\w+)\s+\w+')
      # 第1步 ：['A B','C D']
      # 第2步 ：['A','C']
      print(p2.findall(s))

      p3 = re.compile('(\w+)\s+(\w+)')
      # 第1步 ：['A B','C D']
      # 第2步 ：[('A','B'),('C','D')]
      print(p3.findall(s))
    6、练习
      s = '''<div class="animal">
        <p class="name">
          <a title="Rabbit"></a>
        </p>
        
        <p class="contents">
          Small white rabbit white and white 
        </p>
      </div>'''
      p = re.compile('<div class="animal".*?title="(.*?)">.*?contents">(.*?)</p>',re.S)
      print(p.findall(s))
    7、案例1 ：内涵段子脑筋急转弯抓取
      见 ：04_内涵8脑筋急转弯抓取.py
      网址 ：www.neihan8.com
      步骤：
        1、找URL规律
         第1页:https://www.neihan8.com/njjzw/
         第2页:https://www.neihan8.com/njjzw/index_2.html
         第3页:https://www.neihan8.com/njjzw/index_3.html
        2、用正则匹配出 题目 和 答案
          p = re.compile('<div class="text-.*?title="(.*?)".*?<div class="desc">(.*?)</div>',re.S)
        3、写代码
          1、发请求
          2、用正则匹配
          3、写入本地文件
    8、猫眼电影top100榜单,存到csv表格文件中
      网址：猫眼电影 - 榜单 - top100榜
      目标：抓取电影名、主演、上映时间
      1、知识点讲解
        1、csv模块的使用流程
          1、打开csv文件
            with open("测试.csv","a") as f:
          2、初始化写入对象
            writer = csv.writer(f)
          3、写入数据
            writer.writerow(列表)
        2、示例 见05_csv示例.py
      2、准备工作
        1、找URL
          第1页：http://maoyan.com/board/4?offset=0
          第2页：http://maoyan.com/board/4?offset=10
          第n页：
            offset = (n-1)*10
        2、正则匹配
          <div class="movie-item-info">.*?title="(.*?)".*?<p class="star">(.*?)</p>.*?releasetime">(.*?)</p>
        3、写代码
2、Fiddler常用菜单
  1、Inspector : 查看抓到的数据包的详细内容
    1、分为请求(request)和响应(response)两部分
  2、常用选项
    1、Headers ：显示客户端发送到服务器的header,包含客户端信息、cookie、传输状态
    2、WebForms ：显示请求的POST数据 <body>
    3、Raw ：将整个请求显示为纯文本
4、请求方式及案例
  1、GET
  2、POST
  3、Cookie模拟登陆
    1、什么是cookie、session
      HTTP是一种无连接协议,客户端和服务器交互仅仅限于 请求/响应过程,结束后断开,下一次请求时,服务器会认为是一个新的客户端,为了维护他们之间的连接,让服务器知道这是前一个用户发起的请求,必须在一个地方保存客户端信息。
      cookie ：通过在客户端记录的信息确定用户身份
      session ：通过在服务端记录的信息确定用户身份
    2、案例 ：使用cookie模拟登陆人人网
      见 ：07_cookie模拟登陆人人网.py
      步骤：
        1、通过抓包工具、F12获取到cookie(先登陆1次网站)
        2、正常发请求
          url：http://www.renren.com/967469305/profile
5、requests模块
  1、安装(用管理员身份去打开Anaconda Prompt)
    Anaconda   : conda install requests
    Windows cmd: python -m pip install requests
      ## 以管理员身份去执行pip安装命令
  2、常用方法
    1、get(url,headers=headers) : 发起请求,获取响应对象
    2、response属性
      1、response.text ：返回字符串类型//Unicode型的数据。
      2、response.content : 返回bytes类型也就是二进制的数据
        1、应用场景 ：爬取非结构化数据
          因此如果我们想读取解析文本数据时，使用的是response.text。
          而想读取解析图片文件，往往使用的就是response.content
        2、示例
        3、response.encoding 
          一般返回 ：ISO-8859-1
          response.encoding = "utf-8"
        4、response.status_code ：返回服务器响应码
        5、response.url ：返回数据的URL地址
      3、get()使用场景
        1、没有查询参数
          res = requests.get(url,headers=headers)
        2、有查询参数: params={}
          注 ：params参数必须为字典,自动进行编码
          见 ：09_requests.get.params.py
      4、post() 参数名 ：data
        1、data = {}
        2、示例 ：10_有道翻译post.py
作业：
  1、改写代码,用requests模块实现
  2、链家二手房
    https://bj.lianjia.com/ershoufang/pg1/
  3、百度搜索 链家 -> 二手房
  4、爬取 ：小区名称、总价
        
        

***********************************
Day03笔记
1、requests模块
  1、代理(参数名：proxies)
    1、获取代理IP的网站
      西刺代理网站
      快代理
      全网代理
    2、普通代理
      proxies = {"协议":"协议://IP地址:端口号"}
      183.129.207.82  11597
      183.230.177.118 8060
    3、私密代理
      proxies = {"http":"http://309435365:szayclhp@123.206.119.108:21081"}
  2、案例1 ：爬取链家二手房信息 --> 存到MySQL数据库中
    1、找URL
      https://bj.lianjia.com/ershoufang/pg1/
    2、正则
      <div class="houseInfo".*?data-el="region">(.*?)</a>.*?<div="totalPrice">.*?<span>(.*?)</span>(.*?)</div>
    3、写代码
  3、Web客户端验证(参数名:auth)
    1、auth=("用户名","密码")
       auth=("tarenacode","code_2013")
    2、案例 ：09_Web客户端验证.py
  4、SSL证书认证(参数名:verify)
    1、verify = True : 默认,进行SSL证书认证
    2、verify = False: 不做认证
2、urllib.request中Handler处理器
  1、定义
    自定义的urlopen()方法,urlopen()方法是一个特殊的opener(模块已定义好),不支持代理等功能,通过Handler处理器对象来自定义opener对象
  2、常用方法
    1、build_opener(Handler处理器对象) ：创建opener对象
    2、opener.open(url,参数)
  3、使用流程
    1、创建相关的Handler处理器对象
      http_handler = urllib.request.HTTPHandler()
    2、创建自定义opener对象
      opener = urllib.request.build_opener(http_handler)
    3、利用opener对象打开url
      req = urllib.request.Request(url,headers=headers)
      res = opener.open(req)
  4、Handler处理器分类
    1、HTTPHandler() ：没有任何特殊功能
    2、ProxyHandler(普通代理)
      代理: {"协议":"IP地址:端口号"}
    3、ProxyBasicAuthHandler(密码管理器对象) ：私密代理
    4、HTTPBasicAuthHandler(密码管理器对象) : web客户端认证
    5、密码管理器对象作用
      1、私密代理
      2、Web客户端认证
      3、程序实现流程
        1、创建密码管理器对象
          pwdmg = urllib.request.HTTPPasswordMgrWithDefaultRealm()
        2、把认证信息添加到密码管理器对象
          pwdmg.add_password(None,webserver,user,passwd)
        3、创建Handler处理器对象
          1、私密代理
            proxy = urllib.request.ProxyAuthBasicHandler(pwdmg)
          2、Web客户端
            webbasic = urllib.request.HTTPBasicAuthHandler(pwdmg)
3.安装selenium：
  1、Windows ：安装selenium
    Anaconda Prompt下执行 : 
        python -m pip install selenium
  
  2、Ubuntu ：安装Scrapy框架
    #### 依赖库较多,以下为全部依赖库,有些已安装 ####
    1、sudo apt-get install libssl-dev
       sudo apt-get install libffi-dev 
       sudo apt-get install python3-dev
       sudo apt-get install build-essential
       sudo apt-get install libxml2
       sudo apt-get install libxml2-dev
       sudo apt-get install libxslt1-dev
       sudo apt-get install zlib1g-dev
    2、sudo pip3 install Scrapy


*****************************************************
Day04笔记
1、xpath工具(解析)
  1、xpath
    在XML文档中查找信息的语言,同样适用于HTML文档的检索
  2、xpath辅助工具
    1、Chrome插件 ：XPath Helper
      1、打开 ：Ctrl + Shift + X
      2、关闭 ：Ctrl + Shift + X
    2、Firefox插件 ：XPath checker
    3、XPath表达式编辑工具 ：XML quire
  3、xpath匹配规则
    1、匹配演示
      1、查找bookstore下所有节点：/bookstore
      2、查找所有的book节点：//book
      3、查找所有book下的title节点中,lang属性为"en"的节点
        //book/title[@lang="en"]
      4、查找bookstore下的第2个book节点下的title节点:
        /bookstore/book[2]/title/text()
    2、选取节点
      /  : 从根节点开始选取 
      // : 从整个文档中查找节点
           //price  、  /bookstore/book//price
      @  : 选取某个节点的属性
           //title[@lang="en"]
    3、 @的使用
      1、选取1个节点 ： //title[@lang="en"]
      2、选取N个节点 ： //title[@lang]
      3、选取节点的属性值 : //title/@lang
        <a class=....,src="http://..."

    4、匹配多路径
      1、符号 ： |
      2、获取所有book节点下的 title节点和price节点
        //book/title | //book/price
    5、函数
      1、contains() : 匹配一个属性值中包含某些字符串的节点
      //title[contains(@lang,"e")]
      2、text() 
        //title[contains(@lang,"e")]/text()
    6.xpath 与 正则结合使用
      news_author = response.xpath('//script').re('v.{2}\ss.{4}e\s=\s\"[\u4e00-\u9fa5]+\"')[0][13:].replace('"','')
2、lxml库及xpath使用
  1、lxml库 ：HTML/XML解析库
    1、安装 
      python -m pip install lxml
      conda install lxml
    2、使用流程
      1、导模块
        from lxml import etree
      2、利用lxml库的etree模块创建解析对象
        parseHtml = etree.HTML(html)
      3、解析对象调用xpath工具定位节点信息
        r_list = parseHtml.xpath('xpath表达式')
        ### 只要调用了xpath,结果一定是列表 ###
    3、示例 ：见01_xpath示例.py
    4、如何获取节点对象的内容
      节点对象.text
    5、案例1 ：抓取百度贴吧帖子里面所有的图片
      1、目标 ：抓取指定贴吧所有图片
      2、思路
        1、获取贴吧主页URL,下一页：找URL规律
        2、获取1页中每个帖子的URL
        3、对每个帖子URL发请求,获取帖子里图片URL
        4、对图片URL发请求,以wb方式写入本地文件
      3、步骤
        1、获取贴吧主页URL
          http://tieba.baidu.com/f? + 查询参数
        2、找到页面中所有帖子的URL
          src : 完整链接
          href : 和主URL进行拼接
            /p/5926064184
            http://tieba.baidu.com/p/5926064184
          xpath匹配链接：
           写法1： //div[@class="col2_right j_threadlist_li_right"]/div/div/a/@href
           写法2(推荐): //div[@class="t_con cleafix"]/div/div/div/a/@href
        3、找每个帖子中图片URL
          Xpath匹配：//img[@class="BDE_Image"]/@src
        4、代码实现
    6、案例2 ：糗事百科-xpath
      1、目标 ：用户昵称、段子内容、好笑数、评论数
      2、步骤
        1、找URL
          https://www.qiushibaike.com/8hr/page/1/
        2、xpath匹配
          1、基准xpath：//div[contains(@id,"qiushi_tag_")]

            用户昵称： ./div/a/h2
            段子内容： .//div[@class="content"]/span
            好笑数量： .//i
            评论数量： .//i
3、动态网站数据抓取
  1、Ajax动态加载
    1、特点 ：滚动鼠标混轮时加载
    2、抓包工具 ：查询参数在 WebForms -> QueryString



*****************************
Day05笔记
1、json模块
  1、什么是json?
    javascript中的对象和数组
    对象 ：{key:value}  取值：对象名.key
    数组 ：[...,...] 取值：数组[索引值]
  2、作用
    json格式的字符串 和 Python数据类型 之间的转换
  3、常用方法
    1、json.loads() : json格式 --> Python数据类型
        json      python
        对象      字典
        数组      列表
      见 01_json.loads示例.py
    2、json.dumps() : Python数据类型 --> json格式
        python       json
        字典         对象
        列表         数组
        元组         数组
      ## 注意
        1、json.dumps()默认使用ascii编码
        2、添加参数ensure_ascii=False,禁用ascii编码,##目的是为了输出中文
2、动态网站数据抓取 - Ajax
  1、特点 ：滚动鼠标滑轮时加载
  2、案例 ：豆瓣电影排行榜数据抓取
    1、抓取目标 ：豆瓣电影 - 排行榜 - 剧情 
                  电影名称 、评分
    2、代码实现
3、selenium + phantomjs 强大的网络爬虫组合
  1、selenium
    1、定义 ：Web自动化测试工具,应用于Web自动化测试
    2、特点
      1、可以运行在浏览器,根据指定命令操作浏览器,让浏览器自动加载页面
      2、只是工具,不支持浏览器功能,需要与第三方浏览器结合使用
  2、phantomjs
    1、定义 ：无界面浏览器(无头浏览器)
    2、特点
      1、把网站加载到内存进行页面加载
      2、运行高效
    3、安装
      1、Windows
        1、将下载的可执行文件放到Python安装目录的Scripts目录下C:\Python36\Scripts
      2、Ubuntu
        1、将下载的phantomjs放到一个路径下
        2、添加环境变量：
          vi .bashrc 添加
          export PHANTOM_JS=/home/.../phantomjs-2.1.1-...
          export PATH=$PHANTOM_JS/bin:$PATH
          终端：source .bashrc
          终端：phantomjs
  3、示例代码
    1、示例代码1 : 见05_phantomjs+selenium示例1.py
    2、示例代码2 : 见06_phantomjs+selenium示例2.py
  4、常用方法
    1、driver.get(url)
    2、driver.page_source : 获取响应的html源码
    3、driver.page_source.find("字符串")
      1、作用 ：从html源码中搜索指定字符串
         -1   ：查找失败
       非-1   ：查找成功
    4、单元素查找
      1、driver.find_element_by_id("").text
      2、driver.find_element_by_class_name("")
      3、driver.find_element_by_xpath('xpath表达式')
      4、如果匹配到多个节点,则只返回第1个节点对象
    5、多元素查找
      1、driver.find_elements_by_....
      2、注意
        1、如果结果1个,则返回节点对象,不是列表
        2、如果结果N个,则返回列表
      3、示例见 ：08_driver.find查找元素示例.py
    6、对象名.send_keys("内容")
    7、对象名.click()
  5、案例1 ：登录豆瓣网站
    见 ：09_selenium+Chrome登陆豆瓣案例.py
  6、操作键盘
    1、导模块
      from selenium.webdrier.common.keys import Keys
    2、常用方法
      见 ：10_driver操作键盘示例.py
  7、案例2 ：斗鱼直播网站主播信息抓取(JS分页加载)
    代码见 ：11_斗鱼直播抓取案例.py
    1、抓取目标 ：主播名称 、观众人数
      1、主播 ：class -> dy-name ellipsis fl
        //div[@id="live-list-content"]//span[@class="dy-name ellipsis fl"]

      2、人数 ：class -> dy-num fr
        //div[@id="live-list-content"]//span[@class="dy-num fr"]

      3、下一页按钮(能点) ：class -> shark-pager-next
      4、下一页按钮(不能点)
        class -> shark-pager-next shark-pager-disable shark-pager-disable-next
  8、Chromdriver如何设置无界面模式
    1、opt = webdriver.ChromeOptions()
    2、opt.set_headless()
    3、driver = webdriver.Chrome(options=opt)
    4、driver.get(url)
  9、案例3 ：京东商品爬取
    1、目标
      1、商品名称
      2、商品价格
      3、评论数量
      4、商家名称
    2、代码实现
      


*****************************************
Day06笔记
1、多线程爬虫
  1、进程线程回顾
    1、进程
      1、系统中正在运行的一个应用程序
      2、1个CPU核心1次只能执行1个进程,其他进程处于非运行状态
      3、N个CPU核心可同时执行N个任务
    2、线程
      1、进程中包含的执行单元,1个进程可包含多个线程
      2、线程可使用所属进程空间(1次只能执行1个线程,阻塞)
      3、锁 ：防止多个线程同时使用共享空间
    3、GIL ：全局解释锁
      执行通行证,仅此1个,拿到了通行证可执行,否则 等
    4、应用场景
      1、多进程：大量的密集的计算
      2、多线程：I/O密集
        爬虫 ：网络I/O
        写文件：本次磁盘I/O
  2、案例 ：使用多线程爬取 百思不得其姐 段子
    1、爬取目标 ：段子内容
    2、URL ：http://www.budejie.com/
    3、xpath表达式 
      //div[@class="j-r-list-c-desc"]/a/text()
    4、知识点
      1、队列(from queue import Queue)
        put()
        get()
        Queue.empty() ：是否为空
        Queue.join()  ：如果队列为空,执行其他程序
      2、线程(import threading)
        threading.Thread(target=...)
    5、代码实现 
      见 ：01_多线程爬取百思不得其姐案例.py
2、BeautifulSoup
  1、定义
    HTML或XML的解析器,依赖于lxml
  2、安装
    python -m pip install beautifulsoup4
    导模块：
      from bs4 import BeautifulSoup
  3、使用流程
    1、导入模块
      from bs4 import BeautifulSoup
    2、创建解析对象 
      soup = BeautifulSoup(html,'lxml')
    3、查找节点对象
      soup.find_all(name="属性值")
  4、示例代码 ：见 02_BeautifulSoup示例.py
  5、BeautifulSoup支持的解析库
    1、lxml         ：BeautifulSoup(html,'lxml')
        速度快,文档容错能力强
    2、python标准库 ：BeautifulSoup(html,'html.parser')
        速度一般
    3、xml解析器    ：BeautifulSoup(html,'xml')
        速度快,文档容错能力强
  6、节点选择器
    1、选择节点
      soup.节点名 ：soup.a、soup.ul
    2、获取文本内容
      soup.节点名.string
  7、常用方法
    1、find_all() ：返回列表
      1、r_list = soup.find_all(属性名="属性值")
         r_list = soup.find_all(class_="test")
      2、r_list=soup.find_all("节点名",attrs={"名":"值"})
         r_list=soup.find_all("div",attrs={"class":"test"}
3、Scrapy框架安装(Ubuntu)
  1、安装依赖库
    sudo apt-get install python3-dev build-essential libssl-dev libffi-dev liblxml2 libxml2-dev libxslt1-dev zlib1g-dev
  2、升级pyasn1模块
    sudo pip3 install pyasn1==0.4.4
  3、安装Scrapy
    sudo pip3 install Scrapy
4、Scrapy框架
  1、Scrapy框架
    异步处理框架,可配置和可扩展程度非常高,Python中使用最广泛的爬虫框架
  2、框架组成
    1、引擎(Engine) ：整个框架核心
    2、调度器(Scheduler) ：接受从引擎发过来的URL,入队列
    3、下载器(Downloader)：下载网页源码,返回给爬虫程序
    4、项目管道(Item Pipeline) ：数据处理
    5、下载器中间件(Downloader Middlewares)
      处理引擎与下载器之间的请求与响应
    6、蜘蛛中间件(Spider Middlerwares)
      处理爬虫程序输入响应和输出结果以及新的请求
    7、Item ：定义爬取结果的数据结构,爬取的数据会被赋值为Item对象
  3、Scrapy框架的爬取流程
    见 Scrapy框架流程.png
  4、制作Scrapy爬虫项目步骤
    1、新建项目
      scrapy startproject 项目名
    2、明确目标(items.py)
    3、制作爬虫程序
      XXX/spiders ：scrapy genspider 文件名 域名
    4、处理数据(pipelines.py)
    5、配置settings.py
    6、运行爬虫项目
      scrapy crawl 爬虫名
  5、scrapy项目文件详解
    1、目录结构
      testspider/
      ├── scrapy.cfg   #项目基本配置文件,不用改
      └── testspider
          ├── __init__.py
          ├── items.py       # 定义爬取数据的结构
          ├── middlewares.py # 下载器中间件和蜘蛛中间件实现
          ├── pipelines.py   # 处理数据
          ├── settings.py    # 项目全局配置
          └── spiders        # 存放爬虫程序
            ├── __init__.py
            ├── myspider.py
    2、settings.py配置
      # 是否遵守robots协议,该为False
      ROBOTSTXT_OBEY = False

      # 最大并发量,默认为16个
      CONCURRENT_REQUESTS = 32

      # 下载延迟时间为3秒
      DOWNLOAD_DELAY = 3

      # 请求报头
      DEFAULT_REQUEST_HEADERS = {
         'User-Agent': "Mozilla/5.0",
         'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
         'Accept-Language': 'en',
        }

      # 蜘蛛中间件
      SPIDER_MIDDLEWARES = {
         'testspider.middlewares.TestspiderSpiderMiddleware': 543,
        }

      # 下载器中间件
      DOWNLOADER_MIDDLEWARES = {
         'testspider.middlewares.TestspiderDownloaderMiddleware': 543,
        }

      # 管道文件
      ITEM_PIPELINES = {
         'testspider.pipelines.TestspiderPipeline': 300,
      }
    3、示例 ：抓取百度首页源码,存到baidu.html中
      1、scrapy startproject baidu
      2、cd baidu/baidu
      3、subl items.py(此示例可不用操作)
      4、cd spiders
      5、scrapy genspider baiduspider baidu.com
        # 爬虫名
        # 域名
        # start_urls
        def parse(self,response):
            with open("baidu.html","w") as f:
                f.write(response.text)
      6、subl settings.py
        1、关闭robots协议
        2、添加Headers
      7、cd spiders
      8、scrapy crawl baiduspider
5、pycharm运行scrapy项目
  1、创建文件 begin.py 和 scrapy.cfg同目录
    from scrapy import cmdline
    cmdline.execute("scrapy crawl baiduspider".split())
  2、在 Run -> Editconfigurations -> + -> python
    name : spider
    Script : begin.py的路径
    working directory : 你自己的项目路径
  3、打开begin.py
    右上角 -> 点 运行



*******************************************
Day07笔记
1、生成器
  1、yield作用 ：把一个函数当做一个生成器使用
  2、斐波那契数列 Fib.py
  3、yield特点 ：让函数暂停,等待下1次调用
2、项目 ：Csdn 
  1、知识点 yield 、pipelines.py
  2、目标
    1、https://blog.csdn.net/qq_42231391/article/details/83506181
    2、标题、发表时间、阅读数
    3、步骤
      1、创建项目
      2、定义数据结构(items.py)
      3、创建爬虫程序
      4、第3步抓取的数据通过项目管道去处理
      5、全局配置
      6、运行爬虫程序
3、项目 ：Daomu
  1、URL ：http://www.daomubiji.com/dao-mu-bi-ji-1
  2、目标
    书名、书的标题、章节名称、章节数量、章节链接
  3、步骤
    1、创建项目 Daomu
    2、改items.py(定义数据结构)
    3、创建爬虫文件
    4、改pipelines.py(项目管道文件)
    5、配置settings.py
    6、运行爬虫
4、知识点
  1、extract() : 获取选择器对象中的文本内容
    # response.xpath('...') 得到选择器对象(节点所有内容) [<selector ...,data='<h1>...</h1>']
    # response.xpath('.../text()') 得到选择器对象(节点文本) [<selector ...,data='文本内容'>]
    # extract() : 把选择器对象中的文本取出来 ['文本内容']
  2、爬虫程序中的 start_urls必须为列表
    start_urls = [] 
  3、pipelines.py中必须有1个函数叫
    process_item(self,item,spider),当然还可以写任何其他函数
5、存入MongoDB数据库
  1、在settings.py中定义相关变量
    MONGODB_HOST = 
    MONGODB_PORT = 
  2、可在pipelines.py中新建一个class
    from Daomu import settings
    class DaomumongoPipeline(object):
        def __init__(self):
          host = settings.MONGODB_HOST
  3、在settings.py文件中设置你的项目管道
    ITEM_PIPELINES = {
      "Daomu.pipelines.DaomumongoPipeline":100,
      }
6、存入MySQL数据库
  1、self.db.commit()
7、Csdn项目存到mongodb和mysql
8、腾讯招聘网站案例
  1、URL
    第1页：https://hr.tencent.com/position.php?&start=0
    第2页：https://hr.tencent.com/position.php?&start=10
  2、Xpath匹配
    基准xpath表达式(每个职位节点对象)
      //tr[@class="even"] | //tr[@class="old"]
    职位名称 ： ./td[1]/a/text()
    详情链接 ： ./td[1]/a/@href
    职位类别 ： ./td[2]/text()
    招聘人数 ： ./td[3]/text()
    工作地点 ： ./td[4]/text()
    发布时间 ： ./td[5]/text()
9、设置手机抓包
  1、Fiddler(设置抓包)
  2、在手机上安装证书
    1、手机浏览器打开：http://IP地址:8888 (IP地址是你电脑的IP,8888是Fiddler设置的端口)
    2、在页面上下载(FiddlerRoot certificate)
      下载文件名：FiddlerRoot.cer
    3、直接安装
  3、设置代理
    1、打开手机上已连接的无线, 代理设置 -> 改成 手动
      IP地址：你电脑的IP (ipconfig / ifconfig)
      端口号：8888


********************************
Day08笔记
1、如何设置随机User-Agent
  1、settings.py(少量User-Agent切换,不推荐)
    1、定义USER_AGENT变量值
    2、DEFAULT_REQUEST_HEADER={"User-Agent":"",}
  2、设置中间件的方法来实现
    1、项目目录中新建user_agents.py,放大量Agent
      user_agents = ['','','','','']
    2、middlewares.py写类:
      from 项目名.user_agents import user_agents
      import random
      class RandomUserAgentMiddleware(object):
         def process_request(self,request,spider):
           request.headers["User-Agent"] = random.choice(user_agents)
    3、设置settings.py
      DOWNLOADER_MIDDLEWARES = {"项目名.middlewares.RandomUserAgentMiddleware" : 1}
  3、直接在middlewares.py中添加类
    class RandomUserAgentMiddleware(object):
        def __init__(self):
          self.user_agents = ['','','','','','']
        def process_request(self,request,spider):
          request.header['User-Agent'] = random.choice(self.user_agents)
2、设置代理(DOWNLOADER MIDDLEWARES)
  1、middlewares.py中添加代理中间件ProxyMiddleware
    class ProxyMiddleware(object):
        def process_request(self,request,spider):
            request.meta['proxy'] = "http://180.167.162.166:8080"
  2、settings.py中添加
    DOWNLOADER_MIDDLEWARES = {
       'Tengxun.middlewares.RandomUserAgentMiddleware': 543,
       'Tengxun.middlewares.ProxyMiddleware' : 250,
    }
3、图片管道 ：ImagePipeline
  1、案例 ：斗鱼图片抓取案例(手机app)
    1、菜单 --> 颜值 http://capi.douyucdn.cn/api/v1/getVerticalRoom?limit=20&offset=0
    2、抓取目标  把所有图片保存在/home/tarena/day08/Douyu/Douyu/Images
      1、图片链接
      2、主播名
      3、房间号
      4、城市
    3、步骤
      1、前提 ：手机和电脑为一个局域网
      2、Fiddler抓包工具
        Connections : Allow remote computers to connect
      3、Win+R ：cmd -> ipconfig ->以太网IP地址
      4、配置手机
        手机浏览器 -> http://IP地址:8888
        下载 ：FiddlerRoot certificate
      5、安装
        设置 -> 更多 -> ... -> 从存储设备安装
      6、设置手机代理
        长按Wifi, -> 代理
          IP地址：IP地址
          端口号：端口号
4、ImagePipeline的使用方法
  1、pipelines.py中进行操作
    1、导入模块
      from scrapy.pipelines.images import ImagesPipeline
    2、自定义类,继承 ImagesPipeline
      class DouyuImagePipeline(ImagesPipeline):
        # 重写get_media_requests方法
        def get_media_requests(self,item,info):
          # 向图片URL发起请求,并保存到本地
          yield scrapy.Request(url=item['link'])
  2、settings.py中定义图片存储路径
    IMAGES_STORE = '/home/tarena/Douyu/Images'
5、dont_filter参数：对URL进行去重
  scrapy.Request(url,callback=...,dont_filter=False)
  dont_filter参数 ：False->自动对URL进行去重
                    True -> 不会对URL进行去重
6、Scrapy对接selenium+phantomjs
  1、创建项目 ：Jd
  2、middlewares.py中添加selenium
    1、导模块 ：from selenium import webdriver
    2、定义中间件
      class seleniumMiddleware(object):
           ... 
        def process_request(self,request,info):
          # 注意：参数为request的url
          self.driver.get(request.url)
    3、settings.py
      DOWNLOADER_MIDDLEWARES={"Jd.middleware.seleniumMiddleware":20}
7、Scrapy模拟登陆
  1、创建项目 ：Renren
  2、创建爬虫文件
8、机器视觉与tesseract
  1、OCR(Optical Character Recognition)光学字符识别
    扫描字符 ：通过字符形状 --> 电子文本,OCR有很多的底层识别库
  2、tesseract(谷歌维护的OCR识别开源库,不能import,工具)
    1、安装 
      1、windows下载安装包
        https://sourceforge.net/projects/tesseract-ocr-alt/files/tesseract-ocr-setup-3.02.02.exe/download
        安装完成后添加到环境变量
      2、Ubuntu : suo apt-get install tesseract-ocr
      3、Mac    : brew install tesseract
    2、验证
      终端 ：tesseract test1.jpg text1.txt
  3、安装pytesseract模块
    python -m pip install pytesseract
    # 方法很少,就用1个,图片转字符串：image_to_sting
  4、Python图片的标准库
    from PIL import Image
  5、示例     
    1、验证码图片以wb方式写入到本地
    2、image = Image.open("验证码.jpg")
    3、s = pytesseract.image_to_string(image)
  6、tesseract案例 ：登录豆瓣网站
9、分布式介绍
  1、条件
    1、多台服务器(数据中心、云服务器)
    2、网络带宽
  2、分布式爬虫方式
    1、主从分布式
    2、对等分布式
  3、scrapy-redis

